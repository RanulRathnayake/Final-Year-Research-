{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tetZhag9bsOg",
        "outputId": "fa0240ae-0e97-4cfb-d30b-a5f3c14570d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch scikit-learn pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "VbrR6zIxb1X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/DataSets/FAQs dataset.csv')\n",
        "\n",
        "print(df.head())\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ4Y21azb7cy",
        "outputId": "c969861d-7d62-4684-8867-71564146ca52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              text             entity  \\\n",
            "0               Mata tax return file karanna oney.  income_tax_filing   \n",
            "1            Mata tax return submit karanna oneda?  income_tax_filing   \n",
            "2      Income tax return danna widiyak kiyanawada?  income_tax_filing   \n",
            "3             Tax return submit karanna puluwanda?  income_tax_filing   \n",
            "4  Mata income tax return ekak file karanna oneda?  income_tax_filing   \n",
            "\n",
            "                    intent  \n",
            "0          file_tax_return  \n",
            "1        submit_tax_return  \n",
            "2   how_to_file_tax_return  \n",
            "3    can_submit_tax_return  \n",
            "4  need_to_file_tax_return  \n",
            "Index(['text', ' entity', ' intent'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "entity_encoder = LabelEncoder()\n",
        "intent_encoder = LabelEncoder()\n",
        "\n",
        "df['entity_label'] = entity_encoder.fit_transform(df['entity'])\n",
        "df['intent_label'] = intent_encoder.fit_transform(df['intent'])\n",
        "\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "train_texts, val_texts, train_entity_labels, val_entity_labels, train_intent_labels, val_intent_labels = train_test_split(\n",
        "    df['text'].tolist(),\n",
        "    df['entity_label'].tolist(),\n",
        "    df['intent_label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "0YrR9ez_chMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "F18ZI3S4cipB",
        "outputId": "5e4d6a52-e484-4753-de60-5c6b0996063e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityIntentDataset(Dataset):\n",
        "    def __init__(self, encodings, entity_labels, intent_labels):\n",
        "        self.encodings = encodings\n",
        "        self.entity_labels = entity_labels\n",
        "        self.intent_labels = intent_labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['entity_labels'] = torch.tensor(self.entity_labels[idx], dtype=torch.long)\n",
        "        item['intent_labels'] = torch.tensor(self.intent_labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entity_labels)\n",
        "\n",
        "train_dataset = EntityIntentDataset(train_encodings, train_entity_labels, train_intent_labels)\n",
        "val_dataset = EntityIntentDataset(val_encodings, val_entity_labels, val_intent_labels)\n"
      ],
      "metadata": {
        "id": "pgqECKNrcmFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XLMRobertaForMultiTaskClassification(nn.Module):\n",
        "    def __init__(self, model_name, num_entity_labels, num_intent_labels):\n",
        "        super(XLMRobertaForMultiTaskClassification, self).__init__()\n",
        "        self.roberta = XLMRobertaModel.from_pretrained(model_name)\n",
        "        self.entity_classifier = nn.Linear(self.roberta.config.hidden_size, num_entity_labels)\n",
        "        self.intent_classifier = nn.Linear(self.roberta.config.hidden_size, num_intent_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, entity_labels=None, intent_labels=None):\n",
        "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        entity_logits = self.entity_classifier(pooled_output)\n",
        "        intent_logits = self.intent_classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if entity_labels is not None and intent_labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            entity_loss = loss_fct(entity_logits, entity_labels)\n",
        "            intent_loss = loss_fct(intent_logits, intent_labels)\n",
        "            loss = entity_loss + intent_loss\n",
        "\n",
        "        return {'loss': loss, 'entity_logits': entity_logits, 'intent_logits': intent_logits}\n",
        "\n",
        "model = XLMRobertaForMultiTaskClassification(\n",
        "    'xlm-roberta-base',\n",
        "    num_entity_labels=len(entity_encoder.classes_),\n",
        "    num_intent_labels=len(intent_encoder.classes_)\n",
        ")"
      ],
      "metadata": {
        "id": "P10gW1pAcqLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip install transformers==4.50.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "ke-WYO_f4sYn",
        "outputId": "b43fb36d-f321-4599-95e1-7153efba0def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.50.1\n",
            "Uninstalling transformers-4.50.1:\n",
            "  Successfully uninstalled transformers-4.50.1\n",
            "Collecting transformers==4.50.1\n",
            "  Using cached transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.1) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.1) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.1) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.1) (2025.7.14)\n",
            "Using cached transformers-4.50.1-py3-none-any.whl (10.2 MB)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.50.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "143b3952a7274f06b3669b004289fb54"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efcBVqLdc1M1",
        "outputId": "00557930-cc9c-47e4-b2d8-96851a6bfd3e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "z3q0BkTZc4Vj",
        "outputId": "1b79305c-7a99-480d-8e75-fc94e1519279",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='612' max='612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [612/612 1:31:10, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>9.053900</td>\n",
              "      <td>8.351834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>5.343200</td>\n",
              "      <td>4.953611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.267800</td>\n",
              "      <td>3.270554</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=612, training_loss=6.9832531953948775, metrics={'train_runtime': 5504.8883, 'train_samples_per_second': 0.888, 'train_steps_per_second': 0.111, 'total_flos': 0.0, 'train_loss': 6.9832531953948775, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity_classes = list(entity_encoder.classes_)\n",
        "id2label_entity = {i: label for i, label in enumerate(entity_classes)}\n",
        "label2id_entity = {label: i for i, label in enumerate(entity_classes)}\n",
        "\n",
        "intent_classes = list(intent_encoder.classes_)\n",
        "id2label_intent = {i: label for i, label in enumerate(intent_classes)}\n",
        "label2id_intent = {label: i for i, label in enumerate(intent_classes)}\n",
        "\n",
        "config = {\n",
        "    \"id2label_entity\": id2label_entity,\n",
        "    \"label2id_entity\": label2id_entity,\n",
        "    \"id2label_intent\": id2label_intent,\n",
        "    \"label2id_intent\": label2id_intent,\n",
        "    \"num_entity_labels\": len(entity_encoder.classes_),\n",
        "    \"num_intent_labels\": len(intent_encoder.classes_)\n",
        "}\n"
      ],
      "metadata": {
        "id": "cctr1n9sZWEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import json\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/fine_tuned_xlm_roberta\"\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "# Build label mappings\n",
        "entity_classes = list(entity_encoder.classes_)\n",
        "id2label_entity = {i: label for i, label in enumerate(entity_classes)}\n",
        "label2id_entity = {label: i for i, label in enumerate(entity_classes)}\n",
        "\n",
        "intent_classes = list(intent_encoder.classes_)\n",
        "id2label_intent = {i: label for i, label in enumerate(intent_classes)}\n",
        "label2id_intent = {label: i for i, label in enumerate(intent_classes)}\n",
        "\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), os.path.join(model_path, \"pytorch_model.bin\"))\n",
        "\n",
        "# Save config\n",
        "config = {\n",
        "    \"id2label_entity\": id2label_entity,\n",
        "    \"label2id_entity\": label2id_entity,\n",
        "    \"id2label_intent\": id2label_intent,\n",
        "    \"label2id_intent\": label2id_intent,\n",
        "    \"num_entity_labels\": len(entity_encoder.classes_),\n",
        "    \"num_intent_labels\": len(intent_encoder.classes_)\n",
        "}\n",
        "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "# Save encoders\n",
        "with open(os.path.join(model_path, \"entity_encoder.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(entity_encoder, f)\n",
        "with open(os.path.join(model_path, \"intent_encoder.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(intent_encoder, f)\n",
        "\n",
        "print(f\"Custom model, config, tokenizer, and encoders saved to {model_path}\")\n"
      ],
      "metadata": {
        "id": "5vh27MskyYiW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff0c2e6-b82a-414c-c8e7-4abad0399210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom model, config, tokenizer, and encoders saved to /content/drive/MyDrive/fine_tuned_xlm_roberta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "p6IF5zob0PU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42131da6-b0dd-42b8-b6db-be17ac471fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report, accuracy_score\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "99AFoqVu0dhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, entity_encoder, intent_encoder, device):\n",
        "    model.eval()\n",
        "    all_entity_preds, all_entity_labels = [], []\n",
        "    all_intent_preds, all_intent_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "\n",
        "            inputs = {key: batch[key].to(device) for key in [\"input_ids\", \"attention_mask\"]}\n",
        "            entity_labels = batch[\"entity_labels\"].to(device)\n",
        "            intent_labels = batch[\"intent_labels\"].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            entity_preds = torch.argmax(outputs['entity_logits'], dim=1)\n",
        "            intent_preds = torch.argmax(outputs['intent_logits'], dim=1)\n",
        "\n",
        "            all_entity_preds.extend(entity_preds.cpu().numpy())\n",
        "            all_entity_labels.extend(entity_labels.cpu().numpy())\n",
        "            all_intent_preds.extend(intent_preds.cpu().numpy())\n",
        "            all_intent_labels.extend(intent_labels.cpu().numpy())\n",
        "\n",
        "    entity_precision, entity_recall, entity_f1, _ = precision_recall_fscore_support(\n",
        "        all_entity_labels, all_entity_preds, average=\"weighted\"\n",
        "    )\n",
        "    entity_accuracy = accuracy_score(all_entity_labels, all_entity_preds)\n",
        "\n",
        "    intent_precision, intent_recall, intent_f1, _ = precision_recall_fscore_support(\n",
        "        all_intent_labels, all_intent_preds, average=\"weighted\"\n",
        "    )\n",
        "    intent_accuracy = accuracy_score(all_intent_labels, all_intent_preds)\n",
        "\n",
        "    print(\"\\n **Entity Classification Metrics:**\")\n",
        "    print(f\"Accuracy: {entity_accuracy:.4f}\")\n",
        "    print(f\"Precision: {entity_precision:.4f}\")\n",
        "    print(f\"Recall: {entity_recall:.4f}\")\n",
        "    print(f\"F1-score: {entity_f1:.4f}\")\n",
        "\n",
        "    print(\"\\n **Intent Classification Metrics:**\")\n",
        "    print(f\"Accuracy: {intent_accuracy:.4f}\")\n",
        "    print(f\"Precision: {intent_precision:.4f}\")\n",
        "    print(f\"Recall: {intent_recall:.4f}\")\n",
        "    print(f\"F1-score: {intent_f1:.4f}\")\n",
        "    print(\"\\n \")"
      ],
      "metadata": {
        "id": "oxYk4Dap0im4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "BmbJlcbo01M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WkiBh-hyGo7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17649941-6bd1-4066-eb22-91a2083a32cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaForMultiTaskClassification(\n",
              "  (roberta): XLMRobertaModel(\n",
              "    (embeddings): XLMRobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): XLMRobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): XLMRobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (entity_classifier): Linear(in_features=768, out_features=78, bias=True)\n",
              "  (intent_classifier): Linear(in_features=768, out_features=553, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(\n",
        "    model,\n",
        "    val_dataloader,\n",
        "    entity_encoder,\n",
        "    intent_encoder, device\n",
        "    )"
      ],
      "metadata": {
        "id": "iRMEPcWaNiYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88b8e7e-f746-4b7f-e73e-cedfd5c06d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " **Entity Classification Metrics:**\n",
            "Accuracy: 0.8137\n",
            "Precision: 0.7883\n",
            "Recall: 0.8137\n",
            "F1-score: 0.7818\n",
            "\n",
            " **Intent Classification Metrics:**\n",
            "Accuracy: 0.6912\n",
            "Precision: 0.5554\n",
            "Recall: 0.6912\n",
            "F1-score: 0.6070\n",
            "\n",
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test trained XLM-RoBERTa model using a given text input**"
      ],
      "metadata": {
        "id": "jtqXfnRDI1lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, model, tokenizer, entity_encoder, intent_encoder, device):\n",
        "    model.eval()\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    entity_pred = torch.argmax(outputs['entity_logits'], dim=1).item()\n",
        "    intent_pred = torch.argmax(outputs['intent_logits'], dim=1).item()\n",
        "\n",
        "    entity_label = entity_encoder.inverse_transform([entity_pred])[0]\n",
        "    intent_label = intent_encoder.inverse_transform([intent_pred])[0]\n",
        "\n",
        "    return entity_label, intent_label"
      ],
      "metadata": {
        "id": "Z-dtxio2I_Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/DataSets/fine_tuned_xlm_roberta\"\n",
        "\n",
        "model = XLMRobertaForMultiTaskClassification(\n",
        "    'xlm-roberta-base',\n",
        "    num_entity_labels=len(entity_encoder.classes_),\n",
        "    num_intent_labels=len(intent_encoder.classes_)\n",
        ")\n",
        "model.load_state_dict(torch.load(os.path.join(model_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\")))\n",
        "model.eval()\n",
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(model_path)\n",
        "\n",
        "import pickle\n",
        "with open(os.path.join(model_path, \"entity_encoder.pkl\"), \"rb\") as f:\n",
        "    entity_encoder = pickle.load(f)\n",
        "with open(os.path.join(model_path, \"intent_encoder.pkl\"), \"rb\") as f:\n",
        "    intent_encoder = pickle.load(f)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uYbCDefkJZUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ea50e2-2c37-45a6-f406-d7cd7f21301e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaForMultiTaskClassification(\n",
              "  (roberta): XLMRobertaModel(\n",
              "    (embeddings): XLMRobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): XLMRobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): XLMRobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (entity_classifier): Linear(in_features=768, out_features=78, bias=True)\n",
              "  (intent_classifier): Linear(in_features=768, out_features=553, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"IRD email address eka mokakda?\"\n",
        "entity, intent = predict(text, model, tokenizer, entity_encoder, intent_encoder, device)\n",
        "\n",
        "print(f\" **Text:** {text}\")\n",
        "print(f\" **Predicted Entity:** {entity}\")\n",
        "print(f\" **Predicted Intent:** {intent}\")\n"
      ],
      "metadata": {
        "id": "1DYu5g95JfXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47d309f-3a65-4bb1-ccce-29b845cec2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " **Text:** IRD email address eka mokakda?\n",
            " **Predicted Entity:** Contact_Information_for_IRD_Offices\n",
            " **Predicted Intent:** get_ird_email\n"
          ]
        }
      ]
    }
  ]
}